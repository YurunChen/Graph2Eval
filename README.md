<div align="center">
  <img src="assets/logo4(1).png" alt="Graph2Eval Logo" width="100"/>
</div>

# Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs

<div align="center">

ğŸ“„ [arXiv](https://arxiv.org/abs/2510.00507) | ğŸ¦ [X](https://x.com/YRChen_AIsafety/status/1974361550279549192)

</div> 

## ğŸ¯ Overview

As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.

---

### ğŸ” Key Features

- **ğŸ“š Document Processing**: Automatic document ingestion, cleaning, and chunking
- **ğŸ•¸ï¸ Graph Construction**: Build knowledge graphs from documents using GraphRAG
- **ğŸ¯ Task Generation**: Automatically generate diverse tasks from knowledge graphs
- **ğŸ¤– Multi-Agent System**: Five specialized agents (Planner, Retriever, Reasoner, Verifier, Summarizer)
- **ğŸŒ Web Agent**: Web-based multi-hop task generation and evaluation
- **ğŸ“Š Comprehensive Metrics**: Task success rate, safety compliance, attribution accuracy
- **ğŸ”§ Configurable**: Flexible configuration for different use cases

## ğŸ—ï¸ Agent Architecture

The Graph2Eval system implements multiple agent architectures with flexible RAG capabilities:

### 1. Single Agent Architecture
- **No-RAG Mode**: Direct reasoning without knowledge graph retrieval
- **RAG Mode**: Enhanced with Retrieval-Augmented Generation capabilities

### 2. Multi-Agent System
A collaborative framework with five specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Agent System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Planner Agent  â”‚  Retriever Agent  â”‚  Reasoner Agent       â”‚
â”‚  (Planning)     â”‚  (Information)    â”‚  (Analysis)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Verifier Agent â”‚  Summarizer Agent â”‚                       â”‚
â”‚  (Validation)   â”‚  (Synthesis)      â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Agent Roles:**
- **Planner**: Breaks down complex tasks into manageable steps
- **Retriever**: Searches and retrieves relevant information from knowledge graphs
- **Reasoner**: Performs logical analysis and reasoning
- **Verifier**: Validates answers and checks for consistency
- **Summarizer**: Synthesizes information into final responses

### 3. Web Agent Architecture
Specialized agents for web-based tasks:
- **Agent S 2.5**: Web agent with built-in reflection mechanism for self-improvement ([project](https://github.com/simular-ai/Agent-S))
- **SoM Agent**: Web agent that performs Set-of-marks annotation on input images for precise interaction

### Configuration Options
- **Agent Mode**: `single`, `multi`, `web`
- **Agent Type**: `no_rag`, `rag`, `agent_s`, `som_agent`
- **RAG Integration**: Optional knowledge graph retrieval for all agent types


## ğŸ“– Citation

We would be grateful if you could cite our paper if you find this work helpful for your research:

```bibtex
@misc{chen2025graph2evalautomaticmultimodaltask,
      title={Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs}, 
      author={Yurun Chen and Xavier Hu and Yuhan Liu and Ziqi Wang and Zeyi Liao and Lin Chen and Feng Wei and Yuxi Qian and Bo Zheng and Keting Yin and Shengyu Zhang},
      year={2025},
      eprint={2510.00507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.00507}, 
}
```

If you find this project useful, we would also appreciate a â­ star on GitHub!


## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone git@github.com:YurunChen/Graph2Eval.git
cd Graph2Eval

# Create conda environment
chmod +x setup_environment.sh
./setup_environment.sh 

```
This creates a conda environment named `graphrag-bench`. Activate it:

```bash
conda activate graphrag-bench
```

### 2. Configuration Setup

#### API Keys Configuration

**Option 1: Using .env file (Recommended)**

Create a `.env` file in the root directory by copying from the example file:

```bash
# Copy the example file to create your .env file
cp .env.example .env
```

Example `.env` file:
```bash
# OpenAI API Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1/
OPENAI_ORGANIZATION=your-openai-organization-id

# Anthropic API Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1/

# Hugging Face API Configuration
HUGGINGFACE_API_KEY=your-huggingface-api-key-here
HUGGINGFACE_CACHE_DIR=models/huggingface
```

**Option 2: Configure directly in `configs/main_config.yaml`:**

```yaml
# configs/main_config.yaml
apis:
  # Anthropic API Configuration
  anthropic:
    api_key: your-anthropic-api-key-here
    base_url: https://api.anthropic.com/v1/
  
  # Hugging Face API Configuration
  huggingface:
    api_key: your-huggingface-api-key-here
    cache_dir: models/huggingface
  
  # OpenAI API Configuration
  openai:
    api_key: your-openai-api-key-here
    base_url: https://api.openai.com/v1/
    organization: your-openai-organization-id
```

#### Multi-Agent Configuration

Configure individual agent models in `configs/agent_config.yaml`:

```yaml
# Enable multi-agent system
enable_multi_agent: true

# Multi-agent configuration
multi_agent:
  # Individual LLM models for each agent
  planner_model: gpt-4o-mini
  retriever_model: gpt-4o-mini
  reasoner_model: gpt-4o-mini
  verifier_model: gpt-4o-mini
  summarizer_model: gpt-4o-mini
  
  # System configuration
  enable_parallel_execution: false
  enable_agent_communication: true
  max_iterations: 3
  confidence_threshold: 0.7
  verbose: true
```

### 3. Run Your First Benchmark

The benchmark system supports four main modes: `collect`, `graph`, `generate`, and `evaluate`. Here's how to use each mode:

#### Mode 1: Collect Data from Documents/URLs

```bash
# Collect data from documents
python benchmark_runner.py --mode collect \
    --documents documents/sample.pdf documents/another.pdf

# Or using short form
python benchmark_runner.py -m collect \
    -d documents/sample.pdf documents/another.pdf

# Collect data from web URLs
python benchmark_runner.py --mode collect \
    --urls https://example.com https://httpbin.org/html

# Or using short form
python benchmark_runner.py -m collect \
    -u https://example.com https://httpbin.org/html

# Custom output folder name using -n parameter
python benchmark_runner.py --mode collect \
    --documents documents/sample.pdf \
    --run-name my_custom_collection

# Or using short form
python benchmark_runner.py -m collect \
    -d documents/sample.pdf \
    -n my_custom_collection
```

#### Mode 2: Build Knowledge Graph

```bash
# Build graph from collected data
python benchmark_runner.py --mode graph \
    --collection output/collect/run_*/collections/ \
    --output-dir output/graph/run_*/

# Or using short form
python benchmark_runner.py -m graph \
    -c output/collect/run_*/collections/ \
    -o output/graph/run_*/

# Custom output folder name using -n parameter
python benchmark_runner.py --mode graph \
    --collection output/collect/run_*/collections/ \
    --output-dir output/graph/run_*/ \
    --run-name my_custom_graph

# Or using short form
python benchmark_runner.py -m graph \
    -c output/collect/run_*/collections/ \
    -o output/graph/run_*/ \
    -n my_custom_graph
```

#### Mode 3: Generate Tasks from Graph

```bash
# Generate tasks from knowledge graph
python benchmark_runner.py --mode generate \
    --graph output/graph/run_*/graph/ \
    --output-dir output/generate/run_*/datasets/

# Or using short form
python benchmark_runner.py -m generate \
    -g output/graph/run_*/graph/ \
    -o output/generate/run_*/datasets/

# Custom output folder name using -n parameter
python benchmark_runner.py --mode generate \
    --graph output/graph/run_*/graph/ \
    --output-dir output/generate/run_*/datasets/ \
    --run-name my_custom_tasks

# Or using short form
python benchmark_runner.py -m generate \
    -g output/graph/run_*/graph/ \
    -o output/generate/run_*/datasets/ \
    -n my_custom_tasks
```

#### Mode 4: Evaluate Agent Performance

```bash
# Evaluate on a single dataset file
python benchmark_runner.py --mode evaluate \
    --file output/generate/run_*/datasets/tasks.jsonl

# Or using short form
python benchmark_runner.py -m evaluate \
    -f output/generate/run_*/datasets/tasks.jsonl

# Batch evaluate on multiple datasets
python benchmark_runner.py --mode evaluate \
    --datasets-folder output/generate/run_*/datasets/ \
    --dataset-type all  # normal or all

# Or using short form
python benchmark_runner.py -m evaluate \
    -df output/generate/run_*/datasets/ \
    -t all

# Resume evaluation from existing results
python benchmark_runner.py --mode evaluate \
    --resume output/evaluate/run_20241201_120000/

# Or using short form
python benchmark_runner.py -m evaluate \
    -r output/evaluate/run_20241201_120000/

# Only evaluate existing results (skip execution)
python benchmark_runner.py --mode evaluate \
    --file output/generate/run_*/datasets/tasks.jsonl \
    --evaluate-only

# Or using short form
python benchmark_runner.py -m evaluate \
    -f output/generate/run_*/datasets/tasks.jsonl \
    -eo

# Custom output folder name using -n parameter
python benchmark_runner.py --mode evaluate \
    --file output/generate/run_*/datasets/tasks.jsonl \
    --run-name my_custom_evaluation

# Or using short form
python benchmark_runner.py -m evaluate \
    -f output/generate/run_*/datasets/tasks.jsonl \
    -n my_custom_evaluation
```

#### Complete Workflow Example

```bash
# Step 1: Collect data from documents
python benchmark_runner.py -m collect -d documents/sample.pdf

# Step 2: Build knowledge graph
python benchmark_runner.py -m graph \
    -c output/collect/run_*/collections/ \
    -o output/graph/run_*/

# Step 3: Generate tasks
python benchmark_runner.py -m generate \
    -g output/graph/run_*/graph/ \
    -o output/generate/run_*/datasets/

# Step 4: Evaluate agents
python benchmark_runner.py -m evaluate \
    -df output/generate/run_*/datasets/ \
    -t all
```

## ğŸ“ Project Structure

```
Graph2Eval/
â”œâ”€â”€ ğŸ“‹ benchmark_runner.py          # Main execution script
â”œâ”€â”€ âš™ï¸ config_manager.py            # Configuration management
â”œâ”€â”€ ğŸ“š ingestion/                   # Document processing and web collection
â”‚   â”œâ”€â”€ parsers.py                  # Document parsers
â”‚   â”œâ”€â”€ cleaners.py                 # Text cleaning utilities
â”‚   â”œâ”€â”€ chunkers.py                 # Text chunking strategies
â”‚   â”œâ”€â”€ web_collector.py            # Web data collection
â”‚   â””â”€â”€ tool.py                     # Ingestion tools
â”œâ”€â”€ ğŸ•¸ï¸ graph_rag/                  # Graph construction and storage
â”‚   â”œâ”€â”€ graph_builder.py            # Knowledge graph builder
â”‚   â”œâ”€â”€ embeddings.py               # Vector embeddings
â”‚   â”œâ”€â”€ storage.py                  # Graph storage backends
â”‚   â”œâ”€â”€ node_types.py               # Node type definitions
â”‚   â””â”€â”€ edge_types.py               # Edge type definitions
â”œâ”€â”€ ğŸ¯ task_craft/                 # Task generation and optimization
â”‚   â”œâ”€â”€ task_generator.py           # Main task generator
â”‚   â”œâ”€â”€ task_templates.py           # Task templates
â”‚   â”œâ”€â”€ subgraph_sampler.py         # Subgraph sampling
â”‚   â””â”€â”€ task_coverage_optimizer.py  # Task optimization
â”œâ”€â”€ ğŸ¤– agent_framework/             # Agent execution framework
â”‚   â”œâ”€â”€ agent.py                    # Base agent classes
â”‚   â”œâ”€â”€ multi_agent_system.py       # Multi-agent coordination
â”‚   â”œâ”€â”€ executors.py                # LLM execution
â”‚   â”œâ”€â”€ evaluators.py               # Task evaluation
â”‚   â”œâ”€â”€ retrievers.py               # Knowledge retrieval
â”‚   â””â”€â”€ attributors.py              # Attribution analysis
â”œâ”€â”€ ğŸ—‚ï¸ configs/                    # Configuration files
â”‚   â”œâ”€â”€ main_config.yaml            # Main configuration
â”‚   â”œâ”€â”€ agent_config.yaml           # Agent settings
â”‚   â”œâ”€â”€ task_craft_config.yaml      # Task generation settings
â”‚   â”œâ”€â”€ graph_rag_config.yaml       # Graph construction settings
â”‚   â”œâ”€â”€ datasets_config.yaml        # Dataset configuration
â”‚   â”œâ”€â”€ ingestion_config.yaml       # Data ingestion settings
â”œâ”€â”€ ğŸ“Š output/                     # Results and evaluation outputs
â”‚   â”œâ”€â”€ collect/                    # Data collection results
â”‚   â”œâ”€â”€ graph/                      # Graph construction results
â”‚   â”œâ”€â”€ generate/                   # Task generation results
â”‚   â””â”€â”€ evaluate/                   # Evaluation results
â””â”€â”€ ğŸ§ª logs/                       # Execution logs
```

## ğŸ› ï¸ Configuration

### Core Configuration Files

| File | Purpose |
|------|---------|
| `main_config.yaml` | Main configuration file for benchmark |
| `agent_config.yaml` | Agent execution and multi-agent settings |
| `task_craft_config.yaml` | Task generation parameters |
| `graph_rag_config.yaml` | Graph construction and storage settings |
| `datasets_config.yaml` | Dataset creation and quality control |
| `ingestion_config.yaml` | Document processing and web collection settings |

### Key Configuration Parameters

```yaml
# agent_config.yaml
agent_mode: single  # single, multi, web
agent_type: rag     # no_rag, rag, agent_s, som_agent

# Single agent configuration
single_agent:
  model:
    model_name: gpt-4o-mini
    max_tokens: 4000
    temperature: 0.1
    response_format: structured

# Multi-agent configuration
multi_agent:
  planner_model: gpt-4o-mini
  retriever_model: gpt-4o-mini
  reasoner_model: gpt-4o-mini
  verifier_model: gpt-4o-mini
  summarizer_model: gpt-4o-mini

# task_craft_config.yaml
generation:
  max_total_tasks: 500
  require_gold_answer: true
  require_citations: true
  llm_model_name: "gpt-4o-mini"
  llm_temperature: 0.1
  llm_max_tokens: 4000
  use_llm_quality_check: true
  llm_quality_threshold: 0.7

# datasets_config.yaml
dataset_creation:
  save_format: "jsonl"
  max_total_samples: 5
  min_samples_per_type: 1
  min_quality_score: 0.5
  min_success_rate: 0.3

# graph_rag_config.yaml
graph_builder:
  create_chunk_nodes: true
  create_entity_nodes: true
  semantic_similarity_threshold: 0.7
  chunk_size: 500
  chunk_overlap: 50

storage:
  backend: "json"
  file_path: "output/graph/run_*/graph/knowledge_graph.json"
```

## ğŸ¯ Usage Examples

### Command Line Usage

#### Single Document Processing
```bash
# Process a single PDF document
python benchmark_runner.py -m collect -d documents/research_paper.pdf
python benchmark_runner.py -m graph -c output/collect/run_*/collections/
python benchmark_runner.py -m generate -g output/graph/run_*/graph/
python benchmark_runner.py -m evaluate -f output/generate/run_*/datasets/tasks.jsonl
```

#### Batch Processing Multiple Documents
```bash
# Process multiple documents
python benchmark_runner.py -m collect -d documents/*.pdf documents/*.txt

# Batch evaluate multiple datasets
python benchmark_runner.py -m evaluate \
    -df output/generate/run_*/datasets/ \
    -t all
```

#### Web-based Task Generation
```bash
# Collect data from web URLs
python benchmark_runner.py -m collect -u https://example.com https://httpbin.org/html

# Continue with normal workflow
python benchmark_runner.py -m graph -c output/collect/run_*/collections/
python benchmark_runner.py -m generate -g output/graph/run_*/graph/
python benchmark_runner.py -m evaluate -df output/generate/run_*/datasets/
```

#### Resume and Debug Options
```bash
# Resume interrupted evaluation
python benchmark_runner.py -m evaluate -r output/evaluate/run_20241201_120000/

# Debug mode with verbose logging
python benchmark_runner.py -m evaluate \
    -f output/generate/run_*/datasets/tasks.jsonl \
    --debug

# Only evaluate existing results (skip execution)
python benchmark_runner.py -m evaluate \
    -f output/generate/run_*/datasets/tasks.jsonl \
    -eo
```

### Configuration Examples

#### Single Agent with RAG
```yaml
# agent_config.yaml
agent_mode: single
agent_type: rag
single_agent:
  model:
    model_name: gpt-4o-mini
    max_tokens: 4000
    temperature: 0.1
    response_format: structured
```

#### Multi-Agent System
```yaml
# agent_config.yaml
agent_mode: multi
agent_type: rag
multi_agent:
  planner_model: gpt-4o-mini
  retriever_model: gpt-4o-mini
  reasoner_model: gpt-4o-mini
  verifier_model: gpt-4o-mini
  summarizer_model: gpt-4o-mini
```

#### Web Agent Configuration
```yaml
# agent_config.yaml
agent_mode: web
agent_type: agent_s  # or som_agent
web_agent:
  model:
    model_name: gpt-4o-mini
    max_tokens: 4000
    temperature: 0.1
```

## ğŸ“Š Output Structure

The system creates organized output directories for each mode:

### Collect Mode Output
```
output/collect/run_collect_*/
â”œâ”€â”€ documents/                          # Processed document images
â”‚   â””â”€â”€ images/
â”‚       â”œâ”€â”€ page_1_image_0.png
â”‚       â”œâ”€â”€ page_1_image_1.png
â”‚       â”œâ”€â”€ page_12_image_0.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ results/                            # Collection results
â”‚   â””â”€â”€ document_collection_results.json
â””â”€â”€ web_info/                           # Web collection data (if applicable)
    â””â”€â”€ (web-related files)
```

### Graph Mode Output
```
output/graph/run_graph_*
â”œâ”€â”€ graph/
â”‚   â””â”€â”€ knowledge_graph.json           # Built knowledge graph
â”œâ”€â”€ vectors/                            # Vector embeddings
â”‚   â”œâ”€â”€ vectors_faiss.faiss
â”‚   â”œâ”€â”€ vectors_faiss.metadata
â”‚   â””â”€â”€ vectors_nodes.pkl
â””â”€â”€ results/
    â””â”€â”€ graph_build_results.json
```

### Generate Mode Output
```
output/generate/run_gen_*/
â”œâ”€â”€ datasets/                           # Generated task datasets
â”‚   â””â”€â”€ web_tasks.jsonl
â”œâ”€â”€ graph/                              # Knowledge graph data
â”‚   â””â”€â”€ knowledge_graph.json
â”œâ”€â”€ subgraphs/                          # Subgraph samples
â”‚   â”œâ”€â”€ web_element_centric_unknown_subgraphs.json
â”‚   â””â”€â”€ web_subgraphs_summary.json
â”œâ”€â”€ vectors/                            # Vector embeddings
â”‚   â”œâ”€â”€ vectors_faiss.faiss
â”‚   â”œâ”€â”€ vectors_faiss.metadata
â”‚   â””â”€â”€ vectors_nodes.pkl
â””â”€â”€ results/
    â””â”€â”€ task_generation_results.json
```

### Evaluate Mode Output
```
output/evaluate/run_eval_*/
â””â”€â”€ results/
    â”œâ”€â”€ individual_results/             # Individual task results
    â”‚   â”œâ”€â”€ task_*.json                 # Individual task result files
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ batch_evaluation/               # Batch evaluation results
    â”‚   â”œâ”€â”€ batch_evaluation_results_*.json
    â”‚   â”œâ”€â”€ batch_evaluation_summary_*.json
    â”‚   â”œâ”€â”€ detailed_task_metrics_*.csv
    â”‚   â”œâ”€â”€ overall_metrics_*.csv
    â”‚   â””â”€â”€ task_type_metrics_*.csv
    â”œâ”€â”€ evaluation_results.json         # Main evaluation results
    â”œâ”€â”€ full_res.csv                    # Full results CSV
    â””â”€â”€ summary.csv                     # Summary results
```

### Data Flow Structure

```
output/
â”œâ”€â”€ collect/run_*/collections/          # Collected document data
â”‚   â”œâ”€â”€ documents.jsonl
â”‚   â””â”€â”€ web_data.jsonl
â”œâ”€â”€ graph/run_*/graph/                  # Knowledge graph data
â”‚   â”œâ”€â”€ knowledge_graph.json
â”‚   â””â”€â”€ vectors/
â”‚       â”œâ”€â”€ vectors_faiss.faiss
â”‚       â””â”€â”€ vectors_faiss.metadata
â””â”€â”€ generate/run_*/datasets/            # Generated task datasets
    â”œâ”€â”€ all_tasks.jsonl
    â”œâ”€â”€ normal_tasks.jsonl
    â””â”€â”€ safety_tasks.jsonl
```

## ğŸ¤ Contact

For questions, issues, or contributions:

ğŸ“§ Email: [yurunchen.research@gmail.com](mailto:yurunchen.research@gmail.com)
ğŸ› Issues: [GitHub Issues](https://github.com/YurunChen/Graph2Eval/issues)


---

**â­ Star this repository if you find it helpful!**
